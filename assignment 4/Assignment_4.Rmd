---
title: "HUDM6052 Psychometric II Homework_04"
author: "Chenguang Pan"
date: "2023-11-14"
output:
  pdf_document:
    toc: false
    toc_depth: 4
    number_sections: false
    keep_tex: true
    highlight: tango
---
```{=latex}
\setcounter{tocdepth}{4}
\tableofcontents
```
```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(
  cache = TRUE,
  prompt = TRUE,
  comment = '',
  collapse = TRUE,
  warning = FALSE,
  message = FALSE)
```   

## Q1-a-Parameters-Estimation

*Fit the 1PL, 2PL, and 3PL models...report the estimated item parameters in separated tables*  

**My Solution:**  
To make the layout concise and good-looking, I intentionally omitted the codes for data cleaning and some instant display of running outcomes. I attached the estimated item parameters from all three models into one table to save space.  

```{r}
# load the binary response dataset
library(mirt)
df <- read.csv("/Users/panpeter/Desktop/PhD_Learning/HUDM6052 Psychometric II/HUDM6052_Psychometic_II/assignment 4/binary_reponses.csv")
df <- df[,-1]

# --------------------------------------------------
#                       Run 1PL
# --------------------------------------------------

# specify the model for 33 items loading on 1 dimension
# and constrain all the item slope to be equal for 1PL estimation
spec <- 'F = 1-33
CONSTRAIN = (1-33, a1)'

# estimated the model
# since I constrained all slopes to be equal, here the argument "2PL" is safe
irt_1pl <- mirt(df, model = spec, itemtype = "2PL", SE=T)
irt_1pl

```
```{r, include=FALSE}
# get the estimated parameters
items_1pl <- as.data.frame(coef(irt_1pl, IRTpars=T, simplify=T)$items)
```
```{R}
# --------------------------------------------------
#                       Run 2PL
# --------------------------------------------------
irt_2pl <- mirt(df, model = 1, itemtype = "2PL", SE=T)
irt_2pl
```
```{r, include=FALSE}
# get the estimated parameters
items_2pl <- as.data.frame(coef(irt_2pl, IRTpars=T, simplify=T)$items)
```
```{R}
# --------------------------------------------------
#                       Run 3PL
# --------------------------------------------------
# specify the model
spec <- 'F = 1-33
PRIOR = (1-33, g, norm, -1.1, 2)'
irt_3pl <- mirt(df, model = spec, itemtype = "3PL", SE = T)
irt_3pl
```  

```{r, include=FALSE}
# get the estimated parameters for the 3PL
items_3pl <- as.data.frame(coef(irt_3pl, IRTpars=T, simplify = T)$items)
# attach all estimated parameters from 3 models
items_all <- cbind(items_1pl, items_2pl, items_3pl)
write.csv(items_all, file = "/Users/panpeter/Desktop/PhD_Learning/HUDM6052 Psychometric II/HUDM6052_Psychometic_II/assignment 4/all_estimated_items.csv")
```  
![](all_estimated_items.png)  

Due to the limitation of `mirt package`, I can't constrain all the $\alpha$ to be 1. Rather, I can only set them to be equal across all the items. Therefore, in the estimation for the 1PL, the estimated universal $\alpha$ is .96 here.  
  
  
## Q1-a-(1)

*Does it appear reasonable to assume all the items having an equal slope...*  

**My Solution:**  
No.  
  
  
From a aspect of test development, since this is a test about math placement, we should expect that items can discriminate students with different traits well. In addition, comparing the estimated parameters from `2PL` model versus `1PL`, these items' levels of discrimination spread along a wide range. It is reasonable to have items with higher levels of discrimination than others.  

From a mathematics perspective, since the 1PL model is nested in the 2PL model, I conducted the `Likelihood Ratio Test` to compare the two models as followed: $$D = -2[ln(L_{1pl})-ln(L_{2pl})] .$$  
Plug the log likelihood estimated from the above code chunck, then one can have $D = 363.2$ at the degree of freedom of $df = df_{2pl}-df_{1pl} = 66-34 =32$. Based on the Chi-squared distribution, the p value is lower than .001. Therefore, 2PL is better than 1PL, which means the discrimination is preferred.  

  
## Q1-a-(2)

*Does it appear useful to include a guessing parameter in the model...*  

**My Solution:**  
  
Yes, it is useful.  

Intuitively, it is reasonable to include a guessing parameter since this a test with multiple choice and guessing is very possible. In addition, by looking through all the guessing parameters, one can find that the `matchc13`, `matchc21`, and `matchc32` do have quite high guessing rate, i.e., all above .30.  
  
However, in terms of model comparison, when using the LRT test again to compare the 2PL vs 3PL model, one can have $D = 28.28$ at $33$ degree of freedom, $P = .701$. Based on the parsimony rule, one should endorse the simpler model, i.e., the 2PL.  
  
Therefore, my overall conclusion is including a guessing parameter is useful in this scenario. A practitioner should choose the either model based on their purpose since these two models do not differ a lot.  

  
## Q1-a-(3)

*Evaluate the goodness of fit of the items with the option of the chi-square test...*  

**My Solution:**  
  
I conduct the item fit analysis on each model and summarize the results into one table to make the layout concise.  
```{r}
# get the item fit indices for each model
item_fit_1pl <- itemfit(irt_1pl, na.rm = T)
item_fit_2pl <- itemfit(irt_2pl, na.rm = T)
item_fit_3pl <- itemfit(irt_3pl, na.rm = T)

# combine all the outputs into one table
item_fit_all <- cbind(item_fit_1pl[,c("item")],
                      round(item_fit_1pl[,c("S_X2", "p.S_X2")],3),
                      round(item_fit_2pl[,c("S_X2", "p.S_X2")],3),
                      round(item_fit_3pl[,c("S_X2", "p.S_X2")],3))
names(item_fit_all)[1] <- "item"

# get all the item fit indices for 1PL, 2PL, and 3PL model
item_fit_all
```  
The results show that all items fit well in all models except the `mathc5` item.  

## Q1-a-(4)  

*Evaluate the overall fit of the model. Which model do you prefer for this data?...*  

**My Solution:**  
```{r}
# get the fit indices for 1PL model
M2(irt_1pl, na.rm = T)

# get the fit indices for 2PL model
M2(irt_2pl, na.rm = T)

# get the fit indices for 3PL model
M2(irt_3pl, na.rm = T)
```  
Past studies have recommended that a TLI of or above .95, a CFI of or above .95, an RMSEA of or below .05, and an SRMR of or below .05 could indicate a very good fit. Based on those criteria, all three model demonstrate very good model fit.  
  
Next, by conducting the Likelihood Ratio Test (LRT) on 1PL vs. 2PL and 2PL vs. 3PL (finished in `Q1-a-(1)` and `Q1-a-(2)`), 2PL is preferred since it is significantly different from 1PL. In addition, LRT shows there is no significant difference in 2PL and 3PL. Based on the parsimony rule, I prefer 2PL model on this data.  


